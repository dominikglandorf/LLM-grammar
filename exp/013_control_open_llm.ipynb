{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34434f69-8b6b-4858-9317-7d92058dc678",
   "metadata": {},
   "source": [
    "# Exp13: Control Text Generation with a locally running LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8983f68b-3568-4b5a-b093-140877991c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/scratch_local/mpb672-5191909/tmp/ipykernel_162234/1116210299.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import config\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090c4e7-ccdc-4725-9b7d-e38787ddf8a9",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2313cbcd-9f03-43c7-9c27-1a93bae90fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b3c57-ddc2-49b7-ad99-fc397ef40d0d",
   "metadata": {},
   "source": [
    "Load the model and generate three sentences (as indicated by the end of sequence tokens) and print the longest sentence. This re-ranking will be based on the grammar classifiers later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c5ea76-8610-44b8-a3ab-ac381e3d1887",
   "metadata": {},
   "source": [
    "Load the grammar classifiers from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a887fe89-1518-4988-ac75-35a7e2729d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonlinearTaskHead(torch.nn.Module):\n",
    "    def __init__(self, input_dim, num_labels, hidden_dim=16):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        hidden = self.relu(self.fc1(x))\n",
    "        output = self.classifier(hidden)\n",
    "        return output\n",
    "\n",
    "class MultiTaskBERT(torch.nn.Module):\n",
    "    def __init__(self, bert, task_heads):\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.task_heads = torch.nn.ModuleList(task_heads)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, task_id):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        task_output = self.task_heads[task_id](pooled_output)\n",
    "        return task_output\n",
    "\n",
    "    def forward_all(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        task_outputs = torch.stack(\n",
    "            [torch.argmax(self.task_heads[task_id](pooled_output), dim=1) for task_id in range(len(self.task_heads))],\n",
    "            dim=1\n",
    "        )\n",
    "        return task_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a70b8f3-eebb-4cab-8c62-58834379fe65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('../dat/egp_merged.json')\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', cache_dir=config.CACHE_DIR)\n",
    "backbone_model = BertModel.from_pretrained('bert-base-uncased', cache_dir=config.CACHE_DIR)\n",
    "\n",
    "def load_model(level=\"A1\", max_constructs=500):  \n",
    "    df_level = df[df['Level'] == level]\n",
    "    num_classifiers = min(len(df_level), max_constructs)\n",
    "    task_heads = [NonlinearTaskHead(backbone_model.config.hidden_size, 2) for _ in range(num_classifiers)]\n",
    "    multi_task_model = MultiTaskBERT(copy.deepcopy(backbone_model), task_heads).to(device)\n",
    "    multi_task_model.load_state_dict(torch.load('../models/bert/multi_task_model_state_dict_' + level + '.pth'))\n",
    "    return multi_task_model\n",
    "\n",
    "models = {level: load_model(level) for level in [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\", \"C2\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ed5bcd-3fb3-4828-af97-f0dd2b072991",
   "metadata": {},
   "outputs": [],
   "source": [
    "cefr_texts = pd.read_csv(\"../dat/cefr_leveled_texts.csv\")\n",
    "cefr_texts.head()\n",
    "description = {\n",
    "    \"C2\": \"Can understand and interpret critically virtually all forms of the written language including abstract, structurally complex, or highly colloquial literary and non-literary writings. Can understand a wide range of long and complex texts, appreciating subtle distinctions of style and implicit as well as explicit meaning.\",\n",
    "    \"C1\": \"Can understand in detail lengthy, complex texts, whether or not they relate to his/her own area of speciality, provided he/she can reread difficult sections.\",\n",
    "    \"B2\": \"Can read with a large degree of independence, adapting style and speed of reading to different texts and purposes, and using appropriate reference sources selectively. Has a broad active reading vocabulary, but may experience some difficulty with low-frequency idioms.\",\n",
    "    \"B1\": \"Can read straightforward factual texts on subjects related to his/her field and interest with a satisfactory level of comprehension.\",\n",
    "    \"A2\": \"Can understand short, simple texts on familiar matters of a concrete type which consist of high frequency everyday or job-related language. Can understand short, simple texts containing the highest frequency vocabulary, including a proportion of shared international vocabulary items.\",\n",
    "    \"A1\": \"Can understand very short, simple texts a single phrase at a time, picking up familiar names, words and basic phrases and rereading as required.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19c11caf-0e07-4d6b-9982-d35709b1aabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi!\\nI've been meaning to write for ages and f...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>﻿It was not so much how hard people found the ...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Keith recently came back from a trip to Chicag...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Griffith Observatory is a planetarium, and...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-LRB- The Hollywood Reporter -RRB- It's offici...</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>Light propagating in the vicinity of astrophys...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>Future of dentistry has become one of the most...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>﻿The forests – and suburbs – of Europe are ech...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>Hedge funds are turning bullish on oil once ag...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>Without additional heating, radiative cooling ...</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1494 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text label\n",
       "0     Hi!\\nI've been meaning to write for ages and f...    B2\n",
       "1     ﻿It was not so much how hard people found the ...    B2\n",
       "2     Keith recently came back from a trip to Chicag...    B2\n",
       "3     The Griffith Observatory is a planetarium, and...    B2\n",
       "4     -LRB- The Hollywood Reporter -RRB- It's offici...    B2\n",
       "...                                                 ...   ...\n",
       "1489  Light propagating in the vicinity of astrophys...    C2\n",
       "1490  Future of dentistry has become one of the most...    C2\n",
       "1491  ﻿The forests – and suburbs – of Europe are ech...    C2\n",
       "1492  Hedge funds are turning bullish on oil once ag...    C2\n",
       "1493  Without additional heating, radiative cooling ...    C2\n",
       "\n",
       "[1494 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cefr_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79069ec7-a50d-4138-adad-9bc3ffcdcead",
   "metadata": {},
   "source": [
    "Generate candidates and rank them using the classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37a89801-6711-434f-bd65-4e2d1aacf143",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:55<00:00, 18.53s/it]\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\", torch_dtype=torch.float16, cache_dir=config.CACHE_DIR)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, cache_dir=config.CACHE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18125dca-faf7-4bbc-84c0-ca322ad4f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(level_model, candidates, max_len=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    \n",
    "    for candidate in candidates:\n",
    "        encoding = bert_tokenizer.encode_plus(\n",
    "            candidate,\n",
    "            add_special_tokens=True,\n",
    "            max_length=max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        input_ids.append(encoding['input_ids'].squeeze(0))  # Remove the batch dimension\n",
    "        attention_masks.append(encoding['attention_mask'].squeeze(0))\n",
    "    \n",
    "    input_ids = torch.stack(input_ids).to(device)\n",
    "    attention_masks = torch.stack(attention_masks).to(device)\n",
    "    \n",
    "    return level_model.forward_all(input_ids, attention_mask=attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "623a97ed-07e5-436a-89f0-a1f1c8484487",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_scores(models[\"C2\"], [\"Don't ever touch my belongings without permission!\", \"Don't take another step!\", \"Don't question my authority again!\", \"Don't miss the deadline!\", \"Don't ever talk back to me!\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "172e306a-2e54-4caa-89bf-9546594e7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_candidate(input_ids, max_token_sentence = 64, tok_k=10, eos_chars = [\".\", \"!\", \"?\"]):\n",
    "    generated_tokens = torch.tensor([[]], dtype=torch.int, device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_token_sentence):\n",
    "            next_token_logits = model(torch.cat([input_ids, generated_tokens], dim=1)).logits\n",
    "            probs = torch.nn.functional.softmax(next_token_logits[:, -1, :], dim=-1)\n",
    "            top_k_probs, top_k_indices = torch.topk(probs, tok_k)\n",
    "            renormalized_top_k_probs = top_k_probs / top_k_probs.sum()\n",
    "            top_k_id = torch.multinomial(renormalized_top_k_probs, num_samples=1).item()\n",
    "            next_token_id = top_k_indices[0, top_k_id]\n",
    "            \n",
    "            next_token = tokenizer.decode(next_token_id)\n",
    "            generated_tokens = torch.cat([generated_tokens, torch.tensor([[next_token_id]]).to(device)], dim=1)\n",
    "            #print(generated_tokens)\n",
    "            if any(eos_char in next_token for eos_char in eos_chars):\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "def write_story(level, story, num_candidates=3, max_len = 512, add_info=False):\n",
    "    info = f\", which is described as {description[level]}\" if add_info else \"\"\n",
    "    prompt = f\"<s>[INST] Continue the writing on CEFR level {level}{info}. Do not talk about the CEFR level. [/INST] \"\n",
    "    while len(story) < max_len:\n",
    "        inputs = tokenizer(prompt + story, return_tensors=\"pt\").to(device)\n",
    "        candidates = [generate_candidate(inputs.input_ids) for i in range(num_candidates)]\n",
    "        scores = get_scores(models[level], candidates)\n",
    "        mean_scores = torch.mean(scores.float(),dim=1)\n",
    "        #print(list(zip(candidates, mean_scores)))\n",
    "        story += \" \" + candidates[torch.argmax(mean_scores)]\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08c8ff8d-4a7c-4ff3-9ddb-56a3d06910c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stories = 5\n",
    "num_candidates = 3\n",
    "min_length = 50\n",
    "storyPrompts = cefr_texts.text.apply(lambda text: text[:text.find(' ', min_length)].strip().lstrip('\\ufeff')).unique()\n",
    "random.shuffle(storyPrompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b4cc3339-3bad-4e78-9e27-fb16e6a9fc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14\n",
      "A1\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old. every day, Lucy fed Pirate and gave him water. Pirate purred loudly. He liked to sit on Lucy's lap and be stroked. Lucy loved Pirate very much. She took care of him with kindness. In the evening, Pirate slept next to her in bed. It was cozy and warm. \n",
      "\n",
      "Sometimes, Pirate would play with a red ball. He would bat it around the room with his paw. Lucy watched him play and laughed. Pirate brought her joy. He was a good friend. \n",
      "\n",
      "Lucy took Pirate to the vet for regular check-ups.\n",
      "A2\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old, grey and had a missing eyelid which made him look a little bit scary yet cute. Lucy loved Pirate dearly, and they spent most days together at home. Pirate would often lie in the sun, while Lucy read books or worked on her computer. \n",
      "\n",
      "Lucy's favorite pastime, however, was cooking. She would spend hours in the kitchen, experimenting with new recipes and ingredients. Pirate would sit by her side, occasionally purring and sometimes meowing, asking for a taste of what she was making.\n",
      "B1\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old and had long, grey fur. He was quite a large cat, but gentle and calm. Lucy enjoyed spending time with Pirate, reading books together in the living room or watching the birds from the window. \n",
      "\n",
      "Every morning, Pirate would greet Lucy by rubbing his head against her hand as she sipped her cup of coffee. Then, Pirate would follow her to the bathroom and wait outside the door while she showered. Afterward, he would join her in bed for a cozy nap. \n",
      "\n",
      "One day, as Lucy was preparing dinner, she heard Pirate making strange noises from the basement.\n",
      "B2\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old and had the most unusual markings on his fur. He was a tabby with patches that looked like they had been haphazardly painted on his body, giving him a unique and mysterious appearance. Lucy had adopted Pirate as a kitten, and they had been inseparable ever since. \n",
      "\n",
      "Recently, she had come across an article about feline behavior and was eager to learn more about Pirate. She wanted to know about the different reasons why cats purr, as she had noticed Pirate purring more frequently lately.\n",
      "C1\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old, a majestic black and white tomcat with piercing green eyes and a bushy tail. He was Lucy's most treasured companion, and she loved him dearly. However, Lucy had recently discovered a lump on Pirate's neck. Concerned about her beloved pet, she began to research potential causes. \n",
      "\n",
      "She read through various articles, scholarly papers, and forums dedicated to feline health. Some suggested that the lump could be a simple infection, while others speculated it could be something more sinister, such as cancer.\n",
      "C2\n",
      "Lucy had a cat. His name was Pirate. Pirate was 14 years old, a gray and white tabby with a distinct scar above his right eye, which gave him an air of mystery and danger. Lucy had rescued him from the streets as a kitten, and over the years, they had formed an inseparable bond. \n",
      "\n",
      "However, as Pirate grew older, his health began to decline. He developed arthritis and struggled to move around. Lucy noticed that he was no longer able to jump onto the bed or even climb the steps to his favorite napping spot in the sunroom.\n",
      "____________________________________________________________________________________________________\n",
      "The woman had eight babies at one time. She was famous.\n",
      "A1\n",
      "The woman had eight babies at one time. She was famous. Her name was Octomom. People spoke about her a lot. Octomom had eight babies in a hospital. Each baby was small and needed care. Octomom's children were very special. Octomom took care of them at home. Her family helped her. Octomom's children grew up big and strong. They went to school and played games. Octomom was happy. \n",
      "\n",
      "The man was tall and wore a red suit. He had a long white beard and carried a big bag. His name was Santa Claus. People talked about him every year around Christmas time.\n",
      "A2\n",
      "The woman had eight babies at one time. She was famous. But she wasn't human. this sentence might be found in a text about a unique animal event. The woman, in this context, refers to an elephant. This animal is known to carry and give birth to multiple babies at once. It's a fascinating phenomenon. Elephants are large, intelligent animals found in Africa and Asia. They are the largest living terrestrial animals. \n",
      "\n",
      "The man's job involved working in the kitchen. Every morning, he started by cleaning the dishes.\n",
      "B1\n",
      "The woman had eight babies at one time. She was famous. People from all over the world came to see her and marvel at her miracle. She was known as the \"Sextuplet Mother.\" Her name was Kate McCarthy, and she lived in Ireland. \n",
      "\n",
      "Kate gave birth to six boys and two girls on March 17, 1998. The babies were premature and weighed between 1 pound 10 ounces and 5 pounds 3 ounces. Kate's husband, John, was shocked but happy when he saw his children for the first time. \"I thought I was having triplets or quads, but not eight,\" he said.\n",
      "B2\n",
      "The woman had eight babies at one time. She was famous. Some called her a miracle, others an anomaly. I, however, couldn't help but feel an inkling of awe. I had read about such cases before – octuplets, sextuplets, even quadruplets – but I had never truly grasped the magnitude of having eight infants at once. \n",
      "\n",
      "As I delved deeper into the article, my mind raced with questions. How did she cope? How did she juggle the demands of eight newborns? I scanned the text for answers, my eyes skimming over the lines with newfound agility.\n",
      "C1\n",
      "The woman had eight babies at one time. She was famous. No, she wasn't a multiple birth mother or a surrogate. Octomom, as the media named her, had used reproductive technology to achieve her extraordinary feat. The news spread like wildfire across the globe, leaving people in a state of awe, shock, and disbelief. Many debated whether it was moral or ethical to impregnate a woman with so many embryos at once. Others praised her for her strength and determination, arguing that she deserved the chance to have a large family.\n",
      "C2\n",
      "The woman had eight babies at one time. She was famous. People traveled from far and wide to see this marvel of nature. They wondered how she managed to carry such a large load and care for so many infants all at once. But the truth was, this was no natural occurrence. The woman was none other than the circus star, Anna, who made her living by shocking and astonishing the audience with her reproductive capabilities. \n",
      "\n",
      "Anna's secret was not a magical power or a miracle. It was a carefully planned and executed hoax.\n",
      "____________________________________________________________________________________________________\n",
      "Tracy looked at the flag. The flag is red, white, and\n",
      "A1\n",
      "Tracy looked at the flag. The flag is red, white, and blue. It has stripes and stars. Tracy remembers seeing this flag many times. It is the flag of the United States. Tracy lives in a small town in the United States. She has a red, white, and blue bicycle. Every year, her town holds a big celebration. People wear red, white, and blue clothes and wave flags. They eat hot dogs and apple pie. Tracy likes the celebration. She looks forward to it every year. \n",
      "\n",
      "Tracy went to the store. She wanted to buy some apples.\n",
      "A2\n",
      "Tracy looked at the flag. The flag is red, white, and blue. She could see the stars and stripes. Tracy had learned about this flag in her elementary school. This was the flag of the United States of America. She felt proud to be an American. \n",
      "\n",
      "Tracy's job was to work as a receptionist at a law firm. Every day, she greeted clients and answered the telephone. Some of the clients spoke English as a second language. Tracy tried her best to understand them. She had a small dictionary on her desk to look up new words.\n",
      "B1\n",
      "Tracy looked at the flag. The flag is red, white, and blue, with a bald eagle holding an olive branch and arrows in its claws. She had learned in her history class that this was the national symbol of the United States of America. Tracy was particularly interested in American history and spent hours reading about it online and in books. \n",
      "\n",
      "Today, she came across an article about the significance of the bald eagle in American culture. The text described how the bald eagle became a national symbol. It explained that the bald eagle was chosen because it was a majestic bird, easily spotted, and not hunted for its feathers like other birds.\n",
      "B2\n",
      "Tracy looked at the flag. The flag is red, white, and blue, just like the one in front of her old school. She couldn't help but feel a twinge of nostalgia. Tracy had lived in this country for over a decade, but her heart still belonged to her homeland. She had come to this library to find some information for a project that was due at the local cultural center. The topic was the history of her homeland's flag. \n",
      "\n",
      "Tracy scanned the shelves, looking for a book on the subject. She spotted one entitled \"Flag History of the World.\"\n",
      "C1\n",
      "Tracy looked at the flag. The flag is red, white, and blue, with stripes and stars arranged in a specific pattern. She had seen it countless times before, but today, she felt the need to delve deeper into its meaning. The flag is not just a piece of fabric, but a symbol of a nation and its people. \n",
      "\n",
      "Tracy picked up a history book from the library and began to read. She was determined to understand the intricacies of the flag's design and the story behind it. The book described how the flag had evolved over time, from a simple banner to a symbol of freedom and democracy.\n",
      "C2\n",
      "Tracy looked at the flag. The flag is red, white, and blue, the colors of the Union Jack. But it was not just any flag; it was the Union Jack of the United Kingdom, the symbol of a nation that had shaped her very being. She pondered the intricate design, taking in the subtle nuances of the emblems of Scotland, Wales, and England intertwined. This was not a mere flag, but a representation of history, tradition, and culture. \n",
      "\n",
      "As she traced the intricate patterns and colors of the flag, her mind wandered back to the pages of \"Wuthering Heights.\"\n",
      "____________________________________________________________________________________________________\n",
      "Lois has a headache. The headache started one hour\n",
      "A1\n",
      "Lois has a headache. The headache started one hour ago. Lois takes a pain reliever. She lies down on her bed. Lois closes her eyes and tries to rest. \n",
      "\n",
      "John plays soccer every Saturday. John loves to score goals. He practices every day after school. John wears a red jersey and blue shorts. \n",
      "\n",
      "The sun rises in the east. The sun sets in the west. It is beautiful to watch the sunrise. People watch the sunrise and sunset. \n",
      "\n",
      "Apples are red and sweet. Apples are good for health. An apple a day keeps the doctor away.\n",
      "A2\n",
      "Lois has a headache. The headache started one hour ago, and it's getting worse. She remembers that she forgot to take her prescribed medicine in the morning. Lois decides to take two pills now. \n",
      "\n",
      "The doctor's instructions say to take the medicine with water and food. Lois goes to the kitchen to get a glass of water and something to eat. In the kitchen, she sees an apple on the counter. An apple would be perfect. \n",
      "\n",
      "While Lois is in the kitchen, her husband comes home from work. \"Hi dear,\" he says. \"How was your day?\"\n",
      "B1\n",
      "Lois has a headache. The headache started one hour ago, and it's getting worse by the minute. She remembers having the same feeling last month. This time it came suddenly, without any warning. Lois wonders if it could be a migraine. She decides to search for information online to find out more about this type of headache. \n",
      "\n",
      "She types \"symptoms of migraine\" on her computer and starts to read. The text describes various symptoms such as sensitivity to light and sound, nausea, and dizziness. Lois recognizes many of these symptoms from her past experiences.\n",
      "B2\n",
      "Lois has a headache. The headache started one hour ago, and it's getting worse by the minute. She tried taking some ibuprofen, but it didn't seem to have any effect. Lois decided to take a break from her work and read a book to distract herself. She picked up the novel that she had been reading for the book club next week. \n",
      "\n",
      "As she immersed herself in the story, Lois felt her mind drift away from her headache. The author's words flowed effortlessly, painting vivid images in her mind. Every once in a while, she encountered a word that she wasn't familiar with, but she was able to use the context of the sentence to decipher the meaning.\n",
      "C1\n",
      "Lois has a headache. The headache started one hour ago, but she couldn't quite place the cause. She remembered having a heavy workload at the office, and possibly skimping on meals and sleep over the past few days. Lois decided to take a break from her work and head home for some rest. However, upon reaching home, she found herself feeling increasingly anxious. She tried to read a lengthy and complex article on climate change that she had saved for leisurely reading. \n",
      "\n",
      "The text was dense with technical jargon, diagrams, and complex concepts.\n",
      "C2\n",
      "Lois has a headache. The headache started one hour ago, and she can't seem to shake it off. She tried taking a pain reliever, but it hasn't made a difference. As she sit at her desk, she cannot focus on the report she is drafting for her boss. Frustration builds as she attempts to string together coherent thoughts. In her mind's eye, she sees the words scattered and disjointed like puzzle pieces refusing to fit. \n",
      "\n",
      "But Lois is no novice when it comes to handling challenging tasks. She takes a deep breath and decides to take a different approach.\n",
      "____________________________________________________________________________________________________\n",
      "For many years, gambling in the United States was legal\n",
      "A1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m level \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28mprint\u001b[39m(level)\n\u001b[0;32m----> 8\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mwrite_story\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_candidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(text)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#new_row = {\"label\": level, \"story\": story, \"text\": text}\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#pd.DataFrame([new_row]).to_csv(file_path, mode='a', index=False, header=not os.path.exists(file_path))\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36mwrite_story\u001b[0;34m(level, story, num_candidates, max_len, add_info)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(story) \u001b[38;5;241m<\u001b[39m max_len:\n\u001b[1;32m     24\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt \u001b[38;5;241m+\u001b[39m story, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m [generate_candidate(inputs\u001b[38;5;241m.\u001b[39minput_ids) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_candidates)]\n\u001b[1;32m     26\u001b[0m     scores \u001b[38;5;241m=\u001b[39m get_scores(models[level], candidates)\n\u001b[1;32m     27\u001b[0m     mean_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(scores\u001b[38;5;241m.\u001b[39mfloat(),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 25\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(story) \u001b[38;5;241m<\u001b[39m max_len:\n\u001b[1;32m     24\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt \u001b[38;5;241m+\u001b[39m story, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 25\u001b[0m     candidates \u001b[38;5;241m=\u001b[39m [\u001b[43mgenerate_candidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_candidates)]\n\u001b[1;32m     26\u001b[0m     scores \u001b[38;5;241m=\u001b[39m get_scores(models[level], candidates)\n\u001b[1;32m     27\u001b[0m     mean_scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(scores\u001b[38;5;241m.\u001b[39mfloat(),dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m, in \u001b[0;36mgenerate_candidate\u001b[0;34m(input_ids, max_token_sentence, tok_k, eos_chars)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_token_sentence):\n\u001b[0;32m----> 5\u001b[0m         next_token_logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerated_tokens\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      6\u001b[0m         probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_logits[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m         top_k_probs, top_k_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(probs, tok_k)\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1154\u001b[0m, in \u001b[0;36mMistralForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1151\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1154\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1155\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1166\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1167\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(hidden_states)\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:1039\u001b[0m, in \u001b[0;36mMistralModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1030\u001b[0m         decoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m   1031\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         use_cache,\n\u001b[1;32m   1037\u001b[0m     )\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1039\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1041\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1046\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:754\u001b[0m, in \u001b[0;36mMistralDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    751\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    753\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 754\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    757\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    761\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    762\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    764\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39m_old_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m module\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m/mnt/qb/work/meurers/mpb672/conda_envs/llm2/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:283\u001b[0m, in \u001b[0;36mMistralAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m key_states \u001b[38;5;241m=\u001b[39m repeat_kv(key_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[1;32m    281\u001b[0m value_states \u001b[38;5;241m=\u001b[39m repeat_kv(value_states, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_key_value_groups)\n\u001b[0;32m--> 283\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_weights\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m (bsz, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, q_len, kv_seq_len):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttention weights should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(bsz,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\u001b[38;5;250m \u001b[39mq_len,\u001b[38;5;250m \u001b[39mkv_seq_len)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattn_weights\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    289\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = \"../dat/controlled_generated_texts_mistral.csv\"\n",
    "\n",
    "for story in storyPrompts[:num_stories]:\n",
    "    print(\"_\" * 100)\n",
    "    print(story)\n",
    "    for level in models.keys():\n",
    "        print(level)\n",
    "        text = write_story(level, story, num_candidates, add_info=True)\n",
    "        print(text)\n",
    "        #new_row = {\"label\": level, \"story\": story, \"text\": text}\n",
    "        #pd.DataFrame([new_row]).to_csv(file_path, mode='a', index=False, header=not os.path.exists(file_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99bf977-3797-415d-b717-77d43656c9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
