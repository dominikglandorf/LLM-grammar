{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34434f69-8b6b-4858-9317-7d92058dc678",
   "metadata": {},
   "source": [
    "# Exp13: Control Text Generation with a locally running LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8983f68b-3568-4b5a-b093-140877991c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090c4e7-ccdc-4725-9b7d-e38787ddf8a9",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2313cbcd-9f03-43c7-9c27-1a93bae90fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL=\"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271b3c57-ddc2-49b7-ad99-fc397ef40d0d",
   "metadata": {},
   "source": [
    "Load the model and generate three sentences (as indicated by the end of sequence tokens) and print the longest sentence. This re-ranking will be based on the grammar classifiers later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a89801-6711-434f-bd65-4e2d1aacf143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL, device_map=\"auto\", torch_dtype=torch.float16, cache_dir=\"/mnt/qb/work/meurers/mpb672/cache\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, cache_dir=\"/mnt/qb/work/meurers/mpb672/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "232f2112-daa0-42d0-80de-d0913b89969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"<s>[INST] Continue the writing with language on CEFR level C2. [/INST]\"\"\"\n",
    "story = \"I've been meaning to write for ages\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "172e306a-2e54-4caa-89bf-9546594e7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", but my schedule has been rather hectic lately.\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard.\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard. However, I've finally managed to carve out a few hours for myself, and I'm determined to make the most of it.\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard. However, I've finally managed to carve out a few hours for myself, and I'm determined to make the most of it.\n",
      "\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard. However, I've finally managed to carve out a few hours for myself, and I'm determined to make the most of it.\n",
      "\n",
      "\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard. However, I've finally managed to carve out a few hours for myself, and I'm determined to make the most of it.\n",
      "\n",
      "The autumn leaves are falling gently from the trees, painting the pavement in a beautiful tapestry of reds, oranges, and yellows.\n",
      ", but my schedule has been rather hectic lately. Between juggling work commitments, family obligations, and social engagements, it's been a challenge to find the time and energy to put pen to paper, or rather, fingers to keyboard. However, I've finally managed to carve out a few hours for myself, and I'm determined to make the most of it.\n",
      "\n",
      "The autumn leaves are falling gently from the trees, painting the pavement in a beautiful tapestry of reds, oranges, and yellows. The crisp air carries the scent of woodsmoke and\n"
     ]
    }
   ],
   "source": [
    "eos_chars = [\".\", \"!\", \"?\"]\n",
    "max_len = 512\n",
    "\n",
    "def generate_candidate(input_ids, max_token_sentence = 64):\n",
    "    generated_tokens = torch.tensor([], device=device)\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_token_sentence):\n",
    "            next_token_logits = model(torch.cat(input_ids, generated_tokens)).logits\n",
    "            next_token_id = torch.argmax(next_token_logits[:, -1, :], dim=-1).item()\n",
    "\n",
    "            next_token = tokenizer.decode(next_token_id)\n",
    "            generated_tokens = torch.cat(generated_tokens, torch.tensor([[next_token_id]]).to('cuda'), dim=1)\n",
    "\n",
    "            if any(eos_char in next_token for eos_char in eos_chars):\n",
    "                break\n",
    "\n",
    "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "while len(story) < max_len:\n",
    "    inputs = tokenizer(prompt + story, return_tensors=\"pt\").to(device)\n",
    "    candidates = [generate_candidate(inputs.input_ids)]\n",
    "    print(candidates)\n",
    "    \n",
    "    # TODO: do appropriate candidate selection\n",
    "    story += candidates[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
